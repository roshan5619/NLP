{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZTEk_aXifS5",
        "outputId": "cef6b0da-3862-43bc-910b-c66bdf649996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Ensure required resources are downloaded\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RRldgqIdxJU",
        "outputId": "50a9f01f-061a-4c87-e3ac-7dddf29ca7c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import heapq\n",
        "import re\n",
        "from transformers import pipeline\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "\n",
        "\n",
        "# Sample Text for Summarization\n",
        "text = \"\"\"\n",
        "I’m a data enthusiast and AI practitioner with a strong foundation in deep learning, time series forecasting, and quantum computing.\n",
        "I enjoy turning complex problems into elegant code solutions, leveraging tools like PyTorch, Qiskit, and scikit-learn to drive insights and innovation.\n",
        "Passionate about learning and sharing, I continuously explore the evolving landscape of AI to build meaningful, real-world applications and contribute to the developer community.\n",
        "\"\"\"\n",
        "\n",
        "# Extractive Summarization using NLTK\n",
        "def extractive_summarization(text, num_sentences=2):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    word_frequencies = {}\n",
        "    stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        if word not in stopwords and word.isalnum():\n",
        "            word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
        "\n",
        "    max_freq = max(word_frequencies.values())\n",
        "    for word in word_frequencies:\n",
        "        word_frequencies[word] /= max_freq\n",
        "\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        for word in nltk.word_tokenize(sentence.lower()):\n",
        "            if word in word_frequencies:\n",
        "                sentence_scores[sentence] = sentence_scores.get(sentence, 0) + word_frequencies[word]\n",
        "\n",
        "    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
        "    return ' '.join(summary_sentences)\n",
        "\n",
        "# Abstractive Summarization using Hugging Face Transformer\n",
        "def abstractive_summarization(text):\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "    summary = summarizer(text, max_length=50, min_length=10, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Example for Multi-Document Summarization\n",
        "def get_wikipedia_summary(topic):\n",
        "    url = f\"https://en.wikipedia.org/wiki/{topic}\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    paragraphs = soup.find_all(\"p\")\n",
        "    text = ' '.join([para.text for para in paragraphs[:5]])  # First few paragraphs\n",
        "    return extractive_summarization(text)\n",
        "\n",
        "# Execute and Display Results\n",
        "print(\"Extractive Summary:\")\n",
        "print(extractive_summarization(text))\n",
        "\n",
        "print(\"\\nAbstractive Summary:\")\n",
        "print(abstractive_summarization(text))\n",
        "\n",
        "print(\"\\nMulti-Document Summary Example (Wikipedia - NLP):\")\n",
        "print(get_wikipedia_summary(\"Natural_language_processing\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5p7Co-FizT2",
        "outputId": "350f3b32-4c2c-4c25-e8f6-6078fa0fa260"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extractive Summary:\n",
            "Passionate about learning and sharing, I continuously explore the evolving landscape of AI to build meaningful, real-world applications and contribute to the developer community. \n",
            "I’m a data enthusiast and AI practitioner with a strong foundation in deep learning, time series forecasting, and quantum computing.\n",
            "\n",
            "Abstractive Summary:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I'm a data enthusiast and AI practitioner with a strong foundation in deep learning, time series forecasting, and quantum computing . I enjoy turning complex problems into elegant code solutions, leveraging tools like PyTorch, Qiskit, and sc\n",
            "\n",
            "Multi-Document Summary Example (Wikipedia - NLP):\n",
            "Major tasks in natural language processing are speech recognition, text classification, natural language understanding, and natural language generation. The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-p92vmukA4Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy networkx nltk scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUzRZjOdkBER",
        "outputId": "73f8943f-a19d-4085-b652-d07b08c50dfc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def text_rank_summarization(text, num_sentences=2):\n",
        "    \"\"\"\n",
        "    Summarizes the input text using TextRank (PageRank-based ranking).\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input document.\n",
        "        num_sentences (int): Number of sentences to include in summary.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted summary.\n",
        "    \"\"\"\n",
        "    # 1 Sentence Tokenization\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    # 2️ Compute TF-IDF Vectors\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # 3️ Compute Cosine Similarity Matrix\n",
        "    similarity_matrix = cosine_similarity(X)\n",
        "\n",
        "    # 4️ Create a Graph and Apply PageRank\n",
        "    graph = nx.from_numpy_array(similarity_matrix)\n",
        "    scores = nx.pagerank(graph)\n",
        "\n",
        "    # 5️ Rank Sentences Based on PageRank Scores\n",
        "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "    # 6️ Generate Summary with Top Sentences\n",
        "    summary = \" \".join([s for _, s in ranked_sentences[:num_sentences]])\n",
        "\n",
        "    return summary\n",
        "\n",
        "#  Example Usage\n",
        "text = \"\"\"PageRank is a graph-based ranking algorithm originally developed for ranking web pages.\n",
        "          It has been adapted to NLP tasks such as extractive text summarization.\n",
        "          The algorithm constructs a similarity graph where nodes are text units and edges represent similarity.\n",
        "          It applies a random walk model to rank the most important sentences.\n",
        "          This helps in selecting the most representative information in a document.\"\"\"\n",
        "\n",
        "summary = text_rank_summarization(text, num_sentences=2)\n",
        "\n",
        "print(\" Extracted Summary:\\n\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e60WG2lukX8Z",
        "outputId": "d7a7df9e-be2d-4306-9d6c-de64d6f16f00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Extracted Summary:\n",
            " It applies a random walk model to rank the most important sentences. The algorithm constructs a similarity graph where nodes are text units and edges represent similarity.\n"
          ]
        }
      ]
    }
  ]
}