{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random"
      ],
      "metadata": {
        "id": "IrK2TAOQ5LCi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy corpus (you can replace with a large dataset)\n",
        "sentences = [\n",
        "    \"i am passionate about machine learning\",\n",
        "    \"deep learning inspires me with its potential\",\n",
        "    \"nlp excites me every day\",\n",
        "    \"i enjoy solving problems through coding\",\n",
        "    \"language models fascinate me with their intelligence\",\n",
        "    \"i love working with natural language processing\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "wgOHXgInJUn2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Vocabulary & Tokenization\n",
        "class Vocab:\n",
        "    def __init__(self, texts):\n",
        "        tokens = set(word for sent in texts for word in sent.split())\n",
        "        self.word2idx = {w: i+2 for i, w in enumerate(tokens)}\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        self.word2idx['<unk>'] = 1\n",
        "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "\n",
        "    def encode(self, sent):\n",
        "        return [self.word2idx.get(word, 1) for word in sent.split()]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return ' '.join([self.idx2word[i] for i in ids if i != 0])\n",
        "\n",
        "vocab = Vocab(sentences)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czZ_okRTJdq8",
        "outputId": "bc8946f4-1886-4a77-d9e9-b24e86d8c5f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 32\n",
            "<__main__.Vocab object at 0x7d36158d1350>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sentences, vocab, max_len=8):\n",
        "        self.data = [vocab.encode(s) for s in sentences]\n",
        "        self.data = [s + [0]*(max_len - len(s)) for s in self.data]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx], dtype=torch.long)\n",
        "\n",
        "dataset = TextDataset(sentences, vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrxvUgqJJvz9",
        "outputId": "f713dcfc-5908-461a-e052-6d383708341f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.TextDataset object at 0x7d3615846250>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. VAE Model\n",
        "class TextVAE(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128, latent_dim=32):\n",
        "        super(TextVAE, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.encoder_rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.hidden2mean = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.hidden2logv = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.latent2hidden = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.decoder_rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.outputs2vocab = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def encode(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        _, h = self.encoder_rnn(emb)\n",
        "        h = h.squeeze(0)\n",
        "        return self.hidden2mean(h), self.hidden2logv(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode_stepwise(self, z, max_len=15, temperature=0.8, start_token=\"i\"):\n",
        "        h = self.latent2hidden(z).unsqueeze(0)\n",
        "        start_idx = vocab.word2idx.get(start_token, random.randint(2, vocab.vocab_size - 1))\n",
        "        inputs = torch.tensor([[start_idx]], dtype=torch.long).to(z.device)\n",
        "\n",
        "        outputs = []\n",
        "        prev_token = None\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            emb = self.embedding(inputs)\n",
        "            out, h = self.decoder_rnn(emb, h)\n",
        "            logits = self.outputs2vocab(out[:, -1, :])\n",
        "            probs = F.softmax(logits / temperature, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            next_token_id = next_token.item()\n",
        "\n",
        "            # Avoid repetition and pad\n",
        "            if next_token_id != prev_token and next_token_id != 0:\n",
        "                outputs.append(next_token_id)\n",
        "                prev_token = next_token_id\n",
        "\n",
        "            inputs = next_token\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode_from_z(z, x.size(1))\n",
        "        return x_recon, mu, logvar\n",
        "\n",
        "    def decode_from_z(self, z, seq_len):\n",
        "        h = self.latent2hidden(z).unsqueeze(0)\n",
        "        inputs = torch.full((z.size(0), seq_len), vocab.word2idx['<pad>'], dtype=torch.long).to(z.device)\n",
        "        inputs[:, 0] = random.randint(2, vocab.vocab_size - 1)\n",
        "        emb = self.embedding(inputs)\n",
        "        out, _ = self.decoder_rnn(emb, h)\n",
        "        return self.outputs2vocab(out)"
      ],
      "metadata": {
        "id": "xPaKc3mJJm5k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Loss Function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    recon_loss = F.cross_entropy(recon_x.view(-1, recon_x.size(-1)), x.view(-1), ignore_index=0)\n",
        "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
        "    return recon_loss + kl_div"
      ],
      "metadata": {
        "id": "1r3itEoJJj7U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TextVAE(vocab_size=vocab.vocab_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon, mu, logvar = model(batch)\n",
        "        loss = vae_loss(recon, batch, mu, logvar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbMTW9LBJgYt",
        "outputId": "990b2485-a7cc-42fd-c655-4faa65db0f09"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Loss: 5.1946\n",
            "Epoch 200, Loss: 4.8182\n",
            "Epoch 300, Loss: 4.8544\n",
            "Epoch 400, Loss: 4.6967\n",
            "Epoch 500, Loss: 4.7587\n",
            "Epoch 600, Loss: 4.7767\n",
            "Epoch 700, Loss: 4.7523\n",
            "Epoch 800, Loss: 4.7691\n",
            "Epoch 900, Loss: 4.7370\n",
            "Epoch 1000, Loss: 4.5818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Text Generation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(1, 32).to(device)\n",
        "    print(z)\n",
        "    output_ids = model.decode_stepwise(z, max_len=12, temperature=0.8, start_token=\"language\")\n",
        "    print(\"\\nüìù Generated Sentence:\\n\", vocab.decode(output_ids))\n"
      ],
      "metadata": {
        "id": "PyTAHigH5LFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce1199f1-81f0-40c5-aeae-596cb248b84c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2491,  0.2779, -0.1802,  0.0186,  1.1824, -0.8956,  0.7405, -0.6461,\n",
            "          0.7932, -0.5426, -1.6475,  1.3749, -0.7330, -1.1322, -1.8161,  1.2934,\n",
            "          1.1236, -0.6844,  0.5061, -0.1470, -1.4001,  0.0023, -0.8530,  1.2984,\n",
            "          0.2779, -0.6122,  0.6343, -0.5618,  1.2818, -0.1973, -1.6590,  0.1054]])\n",
            "\n",
            "üìù Generated Sentence:\n",
            " i nlp i nlp i deep i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Sample sentence data\n",
        "sentences = [\n",
        "    \"i enjoy working on ai\",\n",
        "    \"machine learning is fun\",\n",
        "    \"deep learning helps a lot\",\n",
        "    \"natural language is powerful\",\n",
        "    \"models can learn patterns\"\n",
        "]\n",
        "\n",
        "# Vocabulary preparation\n",
        "class Vocab:\n",
        "    def __init__(self, sentences):\n",
        "        tokens = {\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"}\n",
        "        for sentence in sentences:\n",
        "            tokens.update(sentence.split())\n",
        "        self.word2idx = {w: i for i, w in enumerate(sorted(tokens))}\n",
        "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
        "        self.pad = self.word2idx[\"<pad>\"]\n",
        "        self.sos = self.word2idx[\"<sos>\"]\n",
        "        self.eos = self.word2idx[\"<eos>\"]\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "\n",
        "    def encode(self, sentence, max_len=10):\n",
        "        tokens = [self.word2idx.get(w, self.word2idx[\"<unk>\"]) for w in sentence.split()]\n",
        "        tokens = [self.sos] + tokens + [self.eos]\n",
        "        tokens += [self.pad] * (max_len - len(tokens))\n",
        "        return tokens[:max_len]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        words = []\n",
        "        for i in indices:\n",
        "            w = self.idx2word.get(i, \"<unk>\")\n",
        "            if w in {\"<pad>\", \"<sos>\", \"<eos>\"}: continue\n",
        "            words.append(w)\n",
        "        return \" \".join(words)\n",
        "\n",
        "vocab = Vocab(sentences)\n",
        "\n",
        "# Dataset\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, sentences, vocab):\n",
        "        self.data = [vocab.encode(s) for s in sentences]\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.data[idx])\n",
        "        return x, x  # input, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "dataset = SentenceDataset(sentences, vocab)\n",
        "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# VAE Modules\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=64, hidden_dim=128, latent_dim=32):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.hidden2mean = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.hidden2logv = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)\n",
        "        _, h = self.rnn(emb)\n",
        "        mean = self.hidden2mean(h.squeeze(0))\n",
        "        logv = self.hidden2logv(h.squeeze(0))\n",
        "        std = torch.exp(0.5 * logv)\n",
        "        z = mean + std * torch.randn_like(std)\n",
        "        return z, mean, logv\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=64, latent_dim=32, hidden_dim=128, max_len=10):\n",
        "        super().__init__()\n",
        "        self.latent2hidden = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, z, targets=None, teacher_forcing_ratio=0.5):\n",
        "        batch_size = z.size(0)\n",
        "        hidden = self.latent2hidden(z).unsqueeze(0)\n",
        "        input_token = torch.full((batch_size, 1), vocab.sos).to(z.device)\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(self.max_len):\n",
        "            emb = self.embed(input_token)\n",
        "            output, hidden = self.rnn(emb, hidden)\n",
        "            logits = self.fc(output)\n",
        "            outputs.append(logits)\n",
        "            top1 = logits.argmax(2)\n",
        "            input_token = top1 if (targets is None or torch.rand(1).item() > teacher_forcing_ratio) else targets[:, t].unsqueeze(1)\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "# VAE Model\n",
        "class SentenceVAE(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size)\n",
        "        self.decoder = Decoder(vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mean, logv = self.encoder(x)\n",
        "        outputs = self.decoder(z, targets=x)\n",
        "        return outputs, mean, logv\n",
        "\n",
        "# Loss Function\n",
        "def vae_loss(recon, target, mean, logv):\n",
        "    recon = recon.view(-1, vocab.vocab_size)\n",
        "    target = target.view(-1)\n",
        "    CE = F.cross_entropy(recon, target, ignore_index=vocab.pad)\n",
        "    KL = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp()) / mean.size(0)\n",
        "    return CE + KL\n",
        "\n",
        "# Training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SentenceVAE(vocab.vocab_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        recon, mean, logv = model(x)\n",
        "        loss = vae_loss(recon, y, mean, logv)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Sentence Generation\n",
        "def reconstruct(sentence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor([vocab.encode(sentence)]).to(device)\n",
        "        z, _, _ = model.encoder(x)\n",
        "        outputs = model.decoder(z)\n",
        "        tokens = outputs.argmax(2).squeeze(0).tolist()\n",
        "        return vocab.decode(tokens)\n",
        "\n",
        "# Try some test sentences\n",
        "print(\"\\n--- Sentence Reconstruction ---\")\n",
        "for s in sentences:\n",
        "    print(f\"Input     : {s}\")\n",
        "    print(f\"Rewritten : {reconstruct(s)}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k0iBZcB5LIy",
        "outputId": "4b12ef12-8fd9-4972-9275-97fcf34f23e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 4.5043\n",
            "Epoch 20, Loss: 2.2495\n",
            "Epoch 30, Loss: 3.8844\n",
            "Epoch 40, Loss: 2.3272\n",
            "Epoch 50, Loss: 2.1777\n",
            "\n",
            "--- Sentence Reconstruction ---\n",
            "Input     : i enjoy working on ai\n",
            "Rewritten : deep learning helps a lot\n",
            "\n",
            "Input     : machine learning is fun\n",
            "Rewritten : deep learning helps a lot\n",
            "\n",
            "Input     : deep learning helps a lot\n",
            "Rewritten : deep learning helps a lot\n",
            "\n",
            "Input     : natural language is powerful\n",
            "Rewritten : deep learning helps a lot\n",
            "\n",
            "Input     : models can learn patterns\n",
            "Rewritten : deep learning helps a lot\n",
            "\n"
          ]
        }
      ]
    }
  ]
}